---
# 定義報告屬性-YAML metadata

title: "HW2"
subtitle: "summary report for the mushroom dataset"
date: today
author: Tsung-Jiun Huang
format:
 pdf:
    include-in-header:
      - text: |
         \usepackage{setspace,relsize}  %控制行距和字體大小
         \usepackage{geometry}          %設置頁面邊界
         \geometry{verbose,tmargin=3cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}  %邊距(上下左右)
#mainfont: "Microsoft JhengHei UI"        微軟正黑體 UI
#mainfont: "Microsoft JhengHei"           微軟正黑體
mainfont: "Microsoft JhengHei Bold"     # 微軟正黑體加粗
toc: true                      # 是否顯示目錄
lang: zh-Tw
documentclass: article         # 設置 LaTeX 文檔類型:article（標準學術文章格式）、report（較長的報告格式）、book（適合書籍或長篇報告）。
pdf-engine: xelatex            # PDF 編譯引擎:xelatex（支持中文，推薦）
execute:
  tidy: true                   # 格式化輸出
  echo: true                   # 顯示代碼
  warning: false               # 隱藏警告
  message: false               # 隱藏訊息
---
## 一、Data Dictionary
```{r echo=FALSE}
#| label: Data Dictionary
library(tibble)
library(knitr)
library(grid)
library(png)

# 建立 Data Dictionary
data_dict <- tribble(
  ~Variable, ~DataType, ~Definition, ~Note,
  "family", "String", "Name of the family of mushroom species", "Multinomial",
  "name", "String", "Mushroom species name", "Multinomial",
  "class", "Binary", "poisonous=p, edibile=e", "Binary",
  "cap-diameter", "Float, Metric data", "Cap diameter in cm", "[Min, max] or mean",
  "cap-shape", "Nominal data", "Shape of the cap", "bell=b, conical=c, convex=x, flat=f, sunken=s, spherical=p, others=o",
  "cap-surface", "Nominal data", "Surface type of the cap", "fibrous=i, grooves=g, scaly=y, smooth=s, shiny=h, leathery=l, silky=k, sticky=t, wrinkled=w, fleshy=e",
  "cap-color", "Nominal data", "Color of the cap", "brown=n, buff=b, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y, blue=l,  orange=o,  black=k",
  "does-bruise-bleed", "Nominal data", "Bruising or bleeding", "t=yes, f=no",
  "gill-attachment", "Nominal data", "Attachment of the gills", "adnate=a, adnexed=x, decurrent=d, free=e, sinuate=s, pores=p, none=f, unknown=?",
  "gill-spacing", "Nominal data", "Spacing between gills", "close=c, distant=d, none=f",
  "gill-color", "Nominal data", "Color of the gills", "brown=n, buff=b, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y, blue=l,  orange=o,  black=k, none=f",
  "stem-height", "Float, Metric data", "Height of the stem in cm", "[Min, max] or mean",
  "stem-width", "Float, Metric data", "Width of the stem in mm", "[Min, max] or mean",
  "stem-root", "Nominal data", "Root type of the stem", "bulbous=b, swollen=s, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r",
  "stem-surface", "Nominal data", "Surface type of the stem", "fibrous=i, grooves=g, scaly=y, smooth=s, shiny=h, leathery=l, silky=k, sticky=t, wrinkled=w, fleshy=e, none=f",
  "stem-color", "Nominal data", "Color of the stem", "brown=n, buff=b, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y, blue=l,  orange=o,  black=k, none=f",
  "veil-type", "Nominal data", "Type of veil", "p=partial, u=universal",
  "veil-color", "Nominal data", "Color of the veil", "brown=n, buff=b, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y, blue=l,  orange=o,  black=k, none=f",
  "has-ring", "Nominal data", "Presence of a ring", "t=yes, f=no",
  "ring-type", "Nominal data", "Type of ring", "cobwebby=c, evanescent=e, flaring=r, grooved=g, large=l, pendant=p, sheathing=s, zone=z, scaly=y, movable=m, none=f, unknown=?",
  "spore-print-color", "Nominal data", "Color of spore-print", "brown=n, buff=b, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y, blue=l,  orange=o,  black=k",
  "habitat", "Nominal data", "Habitat type", "grasses=g, leaves=l, meadows=m, paths=p, heaths=h, urban=u, waste=w, woods=d",
  "season", "Nominal data", "Season of occurrence", "spring=s, summer=u, autumn=a, winter=w"
)

# 產生 Data Dictionary 表格
kable(data_dict, caption = "Mushroom Dataset Data Dictionary")

# 讀取圖片
img <- readPNG("C:/Users/user/Downloads/mushroom.png")

# 顯示圖片
grid.raster(img)
```
## 二、讀取資料
```{r}
#| label: 載入資料
# R Interface to Python
library(reticulate)               # Make R and Python interoperable, allowing R to call Python code.
use_python("C:/Users/user/anaconda3/python.exe", required = TRUE)  # Finding Anaconda's Python path
# py_config()

library(Hmisc)                    # data analysis and report tools
library(ggplot2)                  # a system for creating graphics
library(tableone)                 # a tool for creating tableone
```

```{python}
#| label: 安裝tableone
# !pip install tableone
```

```{python}
#| label: 安裝各種套件
import re  # Regular expressions (text processing)
import numpy as np  # Numerical computing
import pandas as pd  # Data analysis
import seaborn as sns  # Data visualization
import tensorflow as tf  # Deep learning framework
from tensorflow import keras  # High-level API for TensorFlow
from tableone import TableOne  # Summary tables for data analysis
import matplotlib.pyplot as plt  # Plotting library
from tensorflow.keras import layers, regularizers  # Keras layers & regularization
from sklearn.model_selection import train_test_split  # Split dataset
from sklearn.preprocessing import LabelEncoder  # Encode categorical data
from sklearn.ensemble import RandomForestClassifier  # Machine learning classifier
from sklearn.preprocessing import StandardScaler  # Standardize features
from sklearn.metrics import accuracy_score, classification_report  # Model evaluation
from tensorflow.keras import backend as K  # Low-level TensorFlow operations
```
## 三、資料前處理-轉換為結構化資料
```{python}
#| label: 資料前處理
#| colab: [object Object]
# read CSV 
file_path = "C:/Users/user/Downloads/primary_data.csv"

with open(file_path, "r", encoding="utf-8") as file:
    raw_lines = file.readlines()

# get the feature name
header = raw_lines[0].strip().split(";")  # the first row

# processing data
data = []
for line in raw_lines[1:]:
    values = line.strip().split(";")  # spilt by `;`
    if len(values) != len(header):
        values += [""] * (len(header) - len(values))

    # handing `[]` ，let [10 20] to '10, 20'
    cleaned_values = [re.sub(r"\[|\]", "", v).replace("\t", " ") for v in values]
    data.append(cleaned_values)

# make a DataFrame
df1 = pd.DataFrame(data, columns=header)
```


## 四、資料描述
```{r}
#| label: 讀取資料2
#| results: asis
# read dataset
df <- read.csv("C:/Users/user/Downloads/primary_data_cleaned.csv")
# data description
latex(describe(df), descript = "descriptive statistics", file = '', caption.placement = 'top')
```
Through analysis, it was found that there are 23 different types of families, the most common of which is Tricholoma Family, followed by Russula Family.
There are 2 different families in class, of which poisonous (p) accounts for 55.5% and edible (e) accounts for 44.5%.
cap.diameter has 51 different diameters, 50 of which have different maximum and minimum values, and one that is expressed as an average. The largest diameter is in the range of 2 to 5, followed by 10 to 15.
There are 27 different cap shapes in cap-shape, the most common one is (convex = x) with 48, followed by (convex = x, flat = f) with 29.

## 五、tableone
```{python}
#| label: Table 1設定
import pandas as pd
import numpy as np
from tableone import TableOne

df = pd.read_csv("C:/Users/user/Downloads/primary_data_cleaned.csv")
# define the column in table one
columns = [
    'class',          # 分組變數
    'family',         # 蘑菇科
    'cap-diameter',   # 帽直徑
    'stem-height',    # 莖高度
    'stem-width',     # 莖寬度
    'cap-shape',      # 帽形
    'Cap-surface',    # 帽表面
    'cap-color',      # 帽顏色
    'gill_spacing',   # 鰓間距
    'gill-color',     # 鰓顏色
    'stem-color',     # 莖顏色
    'habitat',        # 棲地
    'season'          # 季節
]

# define the continuous feature
continuous = ['cap-diameter', 'stem-height', 'stem-width']

# define the categorical feature
categorical = [
    'family', 'cap-shape', 'Cap-surface', 'cap-color', 
    'gill_spacing', 'gill-color', 'stem-color', 'habitat', 'season'
]
groupby = 'class'

# processing the value
def convert_range_to_median(value):
    if pd.isna(value):
        return np.nan
    if isinstance(value, str) and ',' in value:
        try:
            values = [float(x.strip()) for x in value.split(',')]
            return np.median(values)
        except:
            return np.nan
    try:
        return float(value)
    except:
        return np.nan

# let data convert to median
for col in continuous:
    df[col] = df[col].apply(convert_range_to_median)

# create Table 1
table = TableOne(
    df, columns=columns, categorical=categorical, continuous=continuous,
    groupby=groupby, missing=True, decimals=2
)
```


```{r}
#| label: 生成 Table 1
library(table1)
df <- read.csv("C:/Users/user/Downloads/primary_data_cleaned.csv")
table1(~ family+gill_attachment+gill_spacing+stem_root+habitat+season| class,data=df)
```

## 六、視覺化圖表分析
```{python}
#| label: class圖表
#| colab: [object Object]

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# read CSV
df = pd.read_csv("C:/Users/user/Downloads/primary_data_cleaned.csv")

# print class's counts
print(df['class'].value_counts())

# make class distribution
plt.figure(figsize=(6, 4))
ax = sns.countplot(data=df, x="class", palette="pastel")
plt.title("Class Distribution")
plt.xlabel("Class")
plt.ylabel("Count")

# add the umber on the chart
for p in ax.patches:
    plt.text(p.get_x() + p.get_width() / 2., p.get_height(), 
             f'{p.get_height():.0f}', 
             ha='center', va='bottom', fontsize=12, color='black')

# show the chart
plt.show()
```
from this chart,we can see that there are 96 poisonous and 77 edibile.

```{r}
#| label: ggpairs
library(GGally)
library(ggplot2)

# read CSV
df <- read.csv("C:/Users/user/Downloads/processed_data11.csv")

# select the numerical_columns
numerical_columns <- sapply(df, is.numeric)

# create a ggpairs chart
plot <- ggpairs(df[, numerical_columns],
                upper = list(continuous = wrap("cor", size = 3)),  # cor-plot size
                lower = list(continuous = wrap("points", alpha = 0.5, size = 0.5)),  # pointsize
                title = "GGpairs Plot") + 
        theme(text = element_text(size = 8))  # textsize

# show the plot
print(plot)
```
```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# select the categorical_columns
categorical_columns = ['family', 'class', 'cap-diameter', 'cap-shape', 'Cap-surface', 'cap-color', 
                       'does-bruise-or-bleed', 'gill_attachment', 'gill_spacing', 'gill-color', 
                       'stem-height', 'stem-width', 'stem-surface', 'stem-color', 'has-ring', 
                       'ring-type', 'habitat', 'season']

# df1 = pd.read_csv("C:/Users/user/Downloads/primary_data_cleaned.csv")

# create a 6×3 chart
fig, axes = plt.subplots(nrows=3, ncols=6, figsize=(30, 15))  # chartsize
axes = axes.flatten()  # Flatten a 2D array to 1D

# create all the chart
for i, column in enumerate(categorical_columns):
    ax = sns.countplot(data=df, x=column, ax=axes[i])
    ax.set_title(f'Frequency of {column}', fontsize=10)  # set title
    ax.tick_params(axis='x', rotation=90, labelsize=8)  # rotation and labelsize

    # add the umber on the chart
    for p in ax.patches:
        ax.annotate(f'{int(p.get_height())}',
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center',
                    fontsize=8, color='black',
                    xytext=(0, 5), textcoords='offset points')

# Adjust layout
plt.tight_layout()

# show the chart
plt.show()
```

## 七、資料前處理-補值+encoding
```{python}
df = pd.read_csv("C:/Users/user/Downloads/primary_data_cleaned.csv")
# calcute the missing values
missing_values = df.isnull().sum()
print("缺失值統計")
print(missing_values[missing_values > 0])
```

```{python}
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer


df=pd.read_csv("C:/Users/user/Downloads/primary_data_cleaned.csv")
# 1. mice impution
numerical_columns = ['cap-diameter', 'stem-height', 'stem-width']  # select the numberical feature

def convert_range_to_median(value):
    if pd.isna(value):
        return np.nan
    if isinstance(value, str) and ',' in value:
        try:
            values = [float(x.strip()) for x in value.split(',')]
            return np.median(values)
        except:
            return np.nan
    try:
        return float(value)
    except:
        return np.nan

# convert to median
for col in numerical_columns:
    df[col] = df[col].apply(convert_range_to_median)

# use MICE stagety
imputer = IterativeImputer(random_state=0)
df[numerical_columns] = imputer.fit_transform(df[numerical_columns])

# Label Encoding
label_columns = ['family', 'class', 'does-bruise-or-bleed', 'gill_attachment', 'has-ring']
label_encoder = LabelEncoder()

for col in label_columns:
    # if it have nan, change it to missing
    df[col] = df[col].fillna('missing')
    df[col] = label_encoder.fit_transform(df[col].astype(str))

# 3. delete the missing feature
columns_to_drop = ['veil-type', 'veil-color', 'Spore-print-color', 'stem-surface', 'stem_root', 'name']
df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)

# 4. One-Hot Encoding
multi_columns = ['cap-shape', 'Cap-surface', 'cap-color', 'gill_spacing', 'gill-color',
                 'stem-color', 'ring-type', 'season', 'habitat']

for col in multi_columns:
    if col in df.columns:
        # fill in nan
        df[col] = df[col].fillna('').astype(str)  # 確保轉成字串
        # split the multi column
        dummies = df[col].str.split(',', expand=True).stack().str.strip()
        dummies = pd.get_dummies(dummies, prefix=col)
        # groupby it
        dummies = dummies.groupby(level=0).sum()
        # concat to DataFrame
        df = pd.concat([df, dummies], axis=1)
        # drop the original column
        df.drop(columns=[col], inplace=True)

# know that how size is it
print("資料大小：", df.shape)

# get the processed_data1
df.to_csv('processed_data1.csv', index=False)
```
I first use MICE (IterativeImputer) to fill in the values. MICE (Multiple Imputation by Chained Equations) can infer missing values based on other numerical features, which is more accurate than simple mean filling.

Then because of cap-diameter, stem-height, stem-width
Some of these numerical features are in range format, so I designed a function convert_range_to_median for subsequent processing. If it is a range (, separated), the median is calculated to fill the value. If it cannot be parsed, it is set to NaN to facilitate subsequent value filling.

Next, we use Label Encoding to process categorical data because they have fewer categories, such as family, class, does-bruise-or-bleed, gill-attachment, has-ring. If there are NaN values, they will be filled with 'missing' to avoid LabelEncoder errors.

Then we remove unimportant features or features with too many missing values, such as: veil-type, veil-color, pore-print-color, stem-surface, stem_root, name → These features may not contain enough information or cannot be directly quantified, so they are removed to simplify the model.

One-Hot Encoding is also used. The processing method for this part is special because some features (such as cap-shape, cap-surface, cap-color, gill-spacing...) may be multi-valued (comma-separated), for example: cap-color, season, etc. Therefore, I did the following steps to fill NaN with "" (empty string) to avoid str.split() errors. str.split(',') splits into multiple categories and then expands into multiple lines (stack()). Use pd.get_dummies() for One-Hot Encoding and convert it to 0/1 features. Use groupby(level=0).sum() to merge multiple rows of results back into the original DataFrame. Delete the original feature field to avoid duplication.

Finally, output the data size (df.shape) to check the final number of features and confirm whether One-Hot Encoding successfully added new features. Save it as processed_data1.csv for subsequent modeling.


## 八、模型訓練比較
```{python}
#| label: 隨機森林模型
#| colab: [object Object]
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# read CSV
df = pd.read_csv("C:/Users/user/Downloads/processed_data1.csv")

# define X and y
X = df.drop(columns=['class'])  # feature
y = df['class']                 # target

# ensure that all is float
X = X.astype(float)

# split the train test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# make a randomforest model
model = RandomForestClassifier(n_estimators=40, random_state=42)

# train the model
model.fit(X_train, y_train)

# predict!
y_pred = model.predict(X_test)

# calculating the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"預測準確度: {accuracy:.4f}")
```
```{python}
#| label: 邏輯回歸模型
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

# read CSV
df = pd.read_csv("C:/Users/user/Downloads/processed_data1.csv")

# define X and y
X = df.drop(columns=['class']).astype(float)
y = df['class']

# make a LogisticRegression model
model = LogisticRegression(max_iter=1000, random_state=42)

# 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

# print the score
print(f"邏輯回歸 5 折交叉驗證準確度: {scores.mean():.4f} (±{scores.std():.4f})")
```


```{python}
#| label: 進階隨機森林模型
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

# read CSV
df = pd.read_csv("C:/Users/user/Downloads/processed_data1.csv")

# define X and y
X = df.drop(columns=['class']).astype(float)
y = df['class']

# make an advanced randomforest model
model = RandomForestClassifier(
    n_estimators=50,      # number of tree
    max_depth=5,         
    min_samples_split=10, 
    random_state=42
)

# 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

# print the score
print(f"輕量化隨機森林 5 折交叉驗證準確度: {scores.mean():.4f} (±{scores.std():.4f})")
```



```{python}
#| label: 進階邏輯回歸模型
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

# read CSV
df = pd.read_csv("C:/Users/user/Downloads/processed_data1.csv")

# define X and y
X = df.drop(columns=['class']).astype(float)
y = df['class']

# make a advanced LogisticRegression model
model = LogisticRegression(max_iter=1000, random_state=42)

# 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

# print the score
print(f"邏輯回歸 5 折交叉驗證準確度: {scores.mean():.4f} (±{scores.std():.4f})")
```

```{python}
#| label: DNN
#| output: false
#| results: hide

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from sklearn.decomposition import PCA  # 用於降維

# read CSV
df = pd.read_csv("C:/Users/user/Downloads/processed_data1.csv")

# define X and y
X = df.drop(columns=['class'])  
y = df['class']  

# StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Dimensionality reduction (reducing the number of features) 
pca = PCA(n_components=30)  # Keep 30 components
X_scaled = pca.fit_transform(X_scaled)
print(f"PCA ：{sum(pca.explained_variance_ratio_):.4f}")

# split the train test data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# make a dnn model
model = keras.Sequential([
    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(X_train.shape[1],)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),

    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),

    layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),

    layers.Dense(1, activation='sigmoid')  # binary question can use sigmoid activation function
])

# use Adam optimizer and setting learning rate
optimizer = keras.optimizers.Adam(learning_rate=0.003)

# compile the model
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# define the reduce function
reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)

# train the model
history = model.fit(X_train, y_train,
                    epochs=200, batch_size=16,
                    validation_data=(X_test, y_test),
                    callbacks=[early_stopping, reduce_lr],
                    verbose=2)
```
```{python}
#| label: DNN-results
#| echo: false
# evaluate the model
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\n✅ 進一步改進後的 Deep Learning Accuracy: {acc:.4f}")

# create the plot
# find loss & accuracy
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# draw the loss curve
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Curve')
plt.legend()

# draw the accuracy curve
plt.subplot(1, 2, 2)
plt.plot(train_acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy Curve')
plt.legend()

plt.show()
```
